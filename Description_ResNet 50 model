# This ResNet50 model is built for classifying and labeling random images through training and testing datasets.
  We can observe the result clearly.


# Data

"label_model" 
: transfer each file folder(5 different SEM image folders) to the 5 different dataset.csv

"combined_labeled_files"
: combine all 5 dataset.csv files to one dataset as "combined_dataset.csv"

"image_paths_labels" 
: labeling the data from "combined_dataset.csv" & save the result as "processed_image_labels.csv"



# Path

1. ResNet50 code script
"/home/nemolinux/Python/sam2/notebooks/Machine Learning model/Model/ML_model_ResNet50"

2. labeled dataset
"/home/nemolinux/Python/sam2/notebooks/Machine Learning model/Model/processed_image_labels.csv"

3. Related dataset files
"/home/nemolinux/Python/sam2/notebooks/Machine Learning model/image/...
"/home/nemolinux/Python/sam2/notebooks/Machine Learning model/Model/...



# Modification

You can modify and customize the code below.



-code line[95]
# Define image transformations
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),
    transforms.RandomResizedCrop(224),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


----------------------------------------------------------------------------------------------------------


-code line[135]
# Define loss function and optimizer
filtered_label_counts = Counter(df_filtered['label'])  # Compute label counts AFTER filtering
class_weights = torch.tensor([1.0 / filtered_label_counts[l] for l in sorted(filtered_label_counts.keys())], dtype=torch.float32).to(device)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, min_lr=1e-6)


----------------------------------------------------------------------------------------------------------


-code line[174]
# Training loop with early stopping
for epoch in range(40):
    model.train()
    total_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}")
    
# You can change the training loop numbers 20, 30, 40... 

----------------------------------------------------------------------------------------------------------

# Result:
The accuracy looks quite acceptable. It would be great if we could develop it.

Epoch 1, Loss: 3.4504
Validation Loss after Epoch 1: 3.2307
Epoch 2, Loss: 2.9377
Validation Loss after Epoch 2: 2.9229
Epoch 3, Loss: 2.5546
Validation Loss after Epoch 3: 2.4075
Epoch 4, Loss: 2.1335
Validation Loss after Epoch 4: 1.7896
Epoch 5, Loss: 1.7886
Validation Loss after Epoch 5: 1.7802
Epoch 6, Loss: 1.5351
Validation Loss after Epoch 6: 1.3218
Epoch 7, Loss: 1.3259
Validation Loss after Epoch 7: 1.1257
Epoch 8, Loss: 1.1569
Validation Loss after Epoch 8: 0.9437
Epoch 9, Loss: 1.0981
Validation Loss after Epoch 9: 0.8096
Epoch 10, Loss: 0.9562
Validation Loss after Epoch 10: 0.9214
Epoch 11, Loss: 0.8611
Validation Loss after Epoch 11: 0.7764
Epoch 12, Loss: 0.7272
Validation Loss after Epoch 12: 0.5716
Epoch 13, Loss: 0.6901
Validation Loss after Epoch 13: 0.6661
Epoch 14, Loss: 0.6923
Validation Loss after Epoch 14: 0.6445
Epoch 15, Loss: 0.7918
Validation Loss after Epoch 15: 0.7604
Epoch 16, Loss: 0.4954
Validation Loss after Epoch 16: 0.6692
Epoch 17, Loss: 0.5661
Validation Loss after Epoch 17: 0.4899
Epoch 18, Loss: 0.6323
Validation Loss after Epoch 18: 0.5058
Epoch 19, Loss: 0.6143
Validation Loss after Epoch 19: 0.6710
Epoch 20, Loss: 0.7125
Validation Loss after Epoch 20: 0.7805
Epoch 21, Loss: 0.5526
Validation Loss after Epoch 21: 0.6400
Epoch 22, Loss: 0.5128
Validation Loss after Epoch 22: 0.7141
Early stopping triggered.
Training Accuracy: 85.16%
Test Accuracy: 72.73%
  


